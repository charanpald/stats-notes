\documentclass{beamer}

\usepackage{beamerthemesplit}
\usepackage[utf8x]{inputenc}
\usepackage{pgf}
\usepackage{default}
\usepackage{url}
\usepackage{subfigure}
\usepackage{algorithmic} 
\usepackage{algorithm} 
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}

\usetheme{Singapore}

\input{../include/CsdMacros}
\graphicspath{{./Figures/}}

\title{Statistics and the Analysis of Data\\ Lecture 5: Probability Distributions Part II}
\author{Charanpal Dhanjal \\ \texttt{charanpal@gmail.com}} 
\institute{\'{E}cole des Ponts}
\date{10th December 2013}

\begin{document}

\frame{\titlepage}


\begin{frame}{Recap}  
\begin{itemize} 
\item Discrete and continuous random variables map sample space to real number 
\item A probability distribution fully describes a random variable 
\item Expectation, variance 
\item Discrete distributions - Bernoulli, Binomial, Geometric, Poisson
\end{itemize}
\end{frame}

\begin{frame}{Outline} 
\begin{itemize} 
 \item Continuous distributions - uniform, exponential and Gaussian
 \item Random vectors 
 \item Convergence of random variables
\end{itemize}
\end{frame}

\begin{frame}{Uniform Distribution}  
 \begin{itemize} 
\item Have already seem the discrete \emph{uniform distribution} e.g. when rolling a fair die. 
\item Taken to the limit, for an interval $[a, b]$ 
\begin{displaymath} 
 p(x) = \frac{1}{b-a}
\end{displaymath}
\item Denoted by $X \sim \mathcal{U}(a, b)$
\item Note that  $X \sim \mathcal{U}(a, b)$ implies $\frac{X - a}{b - a} \sim \mathcal{U}(0, 1)$
\item Expectation is $(a+b)/2$ and variance is $(b-a)^2/12$ 
\end{itemize}
\end{frame}



\begin{frame}{Exponential Distribution} 
\begin{itemize} 
 \item For the bus example what is probability that two buses arrive 20 minutes apart? 
 \item We can model this type of event using the \emph{Exponential distribution} with notation $X \sim \mathcal{E}(\theta)$
 \begin{displaymath}
  p(x) = \left\{ \begin{array}{l l}\frac{1}{\theta}  \exp^{-x/\theta} & x \geq 0 \\ 0 & x < 0 \end{array} \right. 
 \end{displaymath}
  where $\theta$ is a scale parameter. 
\end{itemize}
\end{frame}

\begin{frame}{Exponential Distribution Properties}
\begin{itemize} 
 \item The exponential distribution is memoryless 
 \begin{displaymath}
  P(X > T + s | X > t) = P(X > s)
 \end{displaymath}
\item If $X \sim \mathcal{E}(\theta)$ then $X/\theta \sim \mathcal{E}(1)$
\item The expectation is $\theta$ and variance is $\theta^2$ 
\end{itemize}
\end{frame}


\begin{frame}{Exercise} 
\begin{itemize} 
 \item Show that the variance of the uniform distribution is $(b-a)^2/12$. 
 \item Write down an expression for the cumulative distribution function with a uniform random variable.
 \item Show that the exponential distribution is memoryless. 
\end{itemize}
\end{frame}



\begin{frame}{Normal Distribution}  
\begin{itemize} 
 \item The \emph{normal} or \emph{Gaussian distribution} is common in the real world e.g. test scores of students, heights of adults, speed of pedestrians 
\item $X$ is normally distributed (denoted $\mathcal{N}(\mu, \sigma^2)$) if it has a probability density function 
\begin{displaymath} 
 p(x) = \frac{1}{2\pi\sigma^2}\exp^{-(x-\mu)^2/2\sigma^2} 
\end{displaymath}
where $\mu$ is the mean and $\sigma^2$ is the variance
\item $\mathcal{N}(0, 1)$ is known as the \emph{standard normal distribution} and if $X \sim \mathcal{N}(\mu, \sigma^2)$ then $\frac{X-\mu}{\sigma}$ is standard 
\end{itemize}
\end{frame}

\begin{frame}{Exercise} 
\begin{itemize} 
 \item A light bulb company has found to maximise profits they must design light bulbs to last approximately 1000 hours. Much longer than this and sales are lost as fewer replacement bulbs are required, and shorter lifetimes risk customer dissatisfaction and sales lost to competitors. Assume the life time of the bulbs is normally distributed with a standard deviation of 100. Let $y$ be the number of sales and $x$ be the lifetime of a bulb then these quantities are related by $y = 10000 - 5(x - 1000)$ when $x \geq 1000$ and $y = 10x$ when $x < 1000$. Compute the expected number of sales. 
\end{itemize}
\end{frame}


\begin{frame}{Random Vectors}
\begin{itemize} 
 \item A \emph{random vector} is one composed of random variables $\xv = [\xv_1, \ldots, \xv_n]^T$ where $\xv_i$ are random 
 \item Properties 
 \begin{itemize}
 \item If $\xv_i$ are discrete then $\xv$ is discrete 
 \item We say that $\xv$ is continuous if there exists $p: \mathbb{R}^n \rightarrow [0, \infty)$ such that $P(\xv \in A) = \int_A p(\yv) d \yv $ for all $A$
 \item If all $\xv_i$ are continuous then $\xv$ is not necessarily continuous
 \item If all $\xv_i$ are continuous and independent then $\xv$ is continuous and $p(\xv_1, \ldots, \xv_n) = p_{\xv_1}(\xv_1) \cdots p_{\xv_n}(\xv_n)$ 
 \end{itemize} 
 \item The expectation of a vector is the expectation of its elements  $\mathbb{E}[\xv] = [\mathbb{E}[\xv_1], \ldots, \mathbb{E}[\xv_n]]^T$ 
\end{itemize} 
\end{frame}

\begin{frame}{Covariance}  
\begin{itemize}
 \item The covariance of two random variables $X$ and $Y$ is $\cov(X, Y) = \mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y]$ 
 \item Properties 
 \begin{itemize}
 \item $\cov(X, X) = \var(X)$ 
 \item If $X, Y$ are independent $\cov(X, Y) = 0$ 
 \item $\var(X + Y) = \var(X) + \var(Y) + 2\cov(X, Y)$ 
 \end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}{Covariance Matrices} 
\begin{itemize} 
 \item A covariance matrix $\Cm \in \mathbb{R}^{n \times n}$ models the covariances between all pairs of elements of $\xv$
 \item In other words, $\Cm_{ij} = \cov(\xv_i, \xv_j)$
 \item Properties 
 \begin{itemize}
 \item $\Cm$ is symmetric and positive semi-definite (its eigenvalues are nonnegative) 
 \item If all $\xv_i$ are independent then $\Cm$ is diagonal 
 \item The converse is in general false: if $\Cm$ is diagonal this does not always imply $\xv_i$s are independent 
 \item We can write $\Cm = \mathbb{E}[\xv\xv^T] - \mathbb{E}[\xv]\mathbb{E}[\xv^T]$
 \end{itemize} 
\end{itemize}
\end{frame}

\begin{frame}{Exercise}  
\begin{itemize} 
 \item Show that $\Cm$ is positive semi-definite 
\end{itemize} 
\end{frame}

\begin{frame}{The Multivariate Gaussian}  
\begin{itemize} 
 \item Let $\muv \in \mathbb{R}^n$ be a vector of means, and $\Cm$ be a positive definite covariance matrix. 
 \item We can say that $\xv$ follows a multivariate Gaussian distribution, denoted $\mathcal{N}_n(\muv, C)$, if the density function is 
 \begin{displaymath} 
  p(\yv) = \frac{1}{\sqrt{(2\pi)^2 det(\Cm)}} e^{-\frac{1}{2}(\xv - \muv)^T\Cm^{-1}(\xv - \muv)}  
 \end{displaymath}
 \item Note that $\mathbb{E}[\xv] = \muv$ and $\var(\xv) = \Cm$  
 \item If $\xv_1, \ldots, \xv_n$ are independent Gaussian variables then $\xv$ is Gaussian 
\end{itemize}
\end{frame}

\begin{frame}{Exercise}  
\begin{itemize} 
 \item Let $X_1 \sim \mathcal{N}(\mu_1, \sigma_1^2)$ and $X_2 \sim \mathcal{N}(\mu_2, \sigma_2^2)$ be two independent random variables 
 \item Show that for all $a, b \in \mathbb{R}$ $Y = aX_1 + bX_2$ is Gaussian
 \item What is the expectation and variance of $Y$? 
\end{itemize}
\end{frame}



\begin{frame}{Summary}  
\begin{itemize} 
 \item Some continuous probability distributions - exponential, uniform, normal 
 \item Random vectors, covariance matrices
\end{itemize}
\end{frame}

\end{document}