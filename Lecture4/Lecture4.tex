\documentclass{beamer}

\usepackage{beamerthemesplit}
\usepackage[utf8x]{inputenc}
\usepackage{pgf}
\usepackage{default}
\usepackage{url}
\usepackage{subfigure}
\usepackage{algorithmic} 
\usepackage{algorithm} 
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}

\usetheme{Singapore}

\input{../include/CsdMacros}
\graphicspath{{./Figures/}}

\title{Statistics and the Analysis of Data\\ Lecture 4: Probability Distributions Part I}
\author{Charanpal Dhanjal \\ \texttt{charanpal@gmail.com}} 
\institute{\'{E}cole des Ponts}
\date{26th November 2013}

\begin{document}

\frame{\titlepage}


\begin{frame}{Recap}  
\begin{itemize} 
\item Sample spaces, experiments, events 
\item Independence of events   $P(A \cap B) = P(A)P(B)$
\item Conditional probabilities  $P(A | B) = \frac{P(A \cap B)}{P(B)}$
\item Bayes' Law 
\begin{displaymath}
 P(A | B) = \frac{P(B | A)P(A)}{P(B)}
\end{displaymath}
\end{itemize}
\end{frame}

\begin{frame}{Overview}
\begin{itemize} 
\item Random variables 
\item Probability distributions
\item Expectation, variance 
\item Common discrete distributions 
\begin{itemize}
\item Binomial, Bernoulli, Geometric, Poisson 
\end{itemize}  
\end{itemize}
\end{frame}

\begin{frame}{Discrete Random Variables} 
\begin{itemize} 
 \item More general definition of random variable is any function from sample space to real number $X: \Omega \mapsto \mathbb{R}$ 
\item X is \emph{discrete} if it maps to a finite or countable number of values $X: \Omega \mapsto \{a_1, a_2, \ldots \}$. 
\begin{itemize} 
\item E.g. $X: \Omega \mapsto \{0, 1\}$ for a coin toss 
\end{itemize} 
 \item Another way of thinking about this is if we write $P(X=1, Y=2)$, this is the same as 
 \begin{displaymath}
  P(\{\omega \in \Omega: X(\omega) = 1, Y(\omega) = 2\}) 
 \end{displaymath}
 \end{itemize}
\end{frame}


\begin{frame}{Continuous Random Variables} 
\begin{itemize} 
 \item $X$ is continuous if the following is true 
\begin{displaymath} 
P(\{\omega: X(\omega) \in A\}) = \int_{A} p(x) dx, 
\end{displaymath}
for some interval $A$, where $p$ is a nonnegative \emph{probability density function} 
\end{itemize}
\end{frame}

\begin{frame}{Probability Distributions} 
\begin{itemize}
\item A \emph{probability distribution} defines a random variable 
\item For a discrete variable we have $p_i = P(\{\omega: X(\omega) = i\})$
\begin{itemize}
\item In addition, $\sum_i p_i  = 1$
\item E.g. for a die $a_i = i$, $i=1,\ldots, 6$ and $p_i = 1/6$
\end{itemize}
\item For continuous variable distribution is defined using $p(x)$
\begin{itemize}
\item The probability across the complete range is 1: 
\begin{displaymath}
P(\{\omega: X(\omega) \in \mathbb{R}\}) = \int_\mathbb{R} p(x) dx = 1
\end{displaymath} 
\end{itemize}
\item From a probabilistic standpoint, two variables with the same distribution are identical 
\end{itemize}
\end{frame}

\begin{frame}{Independence}  
\begin{itemize} 
 \item Two variables $X$ and $Y$ are independent if 
 \begin{displaymath} 
  P(X \in A, Y \in B) = P(X \in A)P(Y \in B) \quad \forall A, B
 \end{displaymath}
\item For $n$ variables $X_1, X_2, \ldots, X_n$ independence is 
 \begin{displaymath} 
  P(X_1 \in A_1, \ldots, X_n \in A_n) = P(X_1 \in A_1) \cdots P(X_n \in A_n)
 \end{displaymath}
 for all $A_1, \ldots, A_n$ 
\end{itemize}
\end{frame}

\begin{frame}{Expectation of Discrete Variable} 
\begin{itemize} 
 \item The \emph{expectation} is the analogue of the mean for probability distributions.
 \item Let $X$ be a random variable with domain $\{a_1, a_2, \ldots \}$ and $p_i = P(X = a_i)$ then the expectation is 
 \begin{displaymath} 
  \mathbb{E}[X] = \sum_{i=1}^\infty a_i p_i
 \end{displaymath}
\end{itemize}
\end{frame}

\begin{frame}{Expectation of Continuous Variable} 
\begin{itemize} 
 \item The expectation of a continuous variable $X$ with density $p(x)$ is (assuming the integral exists) 
 \begin{displaymath} 
  \mathbb{E}[X] = \int_{-\infty}^\infty x p(x) dx
 \end{displaymath}
\end{itemize}
\end{frame}

\begin{frame}{Variance and Standard Deviation} 
\begin{itemize} 
 \item If $X^2$ is integrable we say that $X$ is \emph{square integrable} and the variance is defined as follows 
 \begin{displaymath} 
 \var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2  
 \end{displaymath}
 \item The standard deviation is $\sqrt{\var(X)}$
 \item Note that expectation and variance depend only on the probability distribution 
\end{itemize}
\end{frame}

\begin{frame}{Properties of Expectation} 
\begin{itemize} 
 \item Let $X$ and $Y$ be integrable random functions and $\lambda, \rho \in \mathbb{R}$ be constants
 \begin{itemize}
 \item Linearity: $\mathbb{E}[X + \lambda Y + \rho] = \mathbb{E}[X] +  \lambda \mathbb{E}[Y] + \rho$ 
 \item Positivity: $P(X \geq 0) = 1$ implies $\mathbb{E}[X] \geq 0$ 
 \item If $P(X \leq Y) = 1$ then $\mathbb{E}[X] \leq \mathbb{E}[Y]$
\item If $X$ is discrete and $f: \mathbb{R} \mapsto \mathbb{R}$ then $f(X)$ is integrable iff $\sum_i |f(a_i)| p_i < + \infty$ and $\mathbb{E}[f(X)] =  \sum_i f(a_i) p_i$ 
\item IF $X$ is continuous $f: \mathbb{R} \mapsto \mathbb{R}$ then $f(X)$ is integrable iff $\int_\mathbb{R} |f(x)| p(x) dx < +\infty$ and $\mathbb{E}[f(X)] = \int_\mathbb{R} f(x)p(x) dx $ 
\end{itemize} 
\end{itemize} 
\end{frame}

\begin{frame}{Properties of Variance} 
\begin{itemize} 
 \item For all $X$ we have $\var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$ 
\item If $X_1, \ldots X_n$ are square integrable and independent then $\var[\sum_{i=1}^n X_i] = \sum_{i=1}^n \var(X_i)$ 
\end{itemize}
\end{frame}

\begin{frame}{Exercise}  
\begin{itemize}
 \item Prove from  $\var(X) = \mathbb{E}[(X - \mathbb{E}[X])^2]$ that $\var(X) = \mathbb{E}[X^2] - (\mathbb{E}[X])^2$ 
 \item The maximal height rise of a river during the period of a year is modelled by a random variable $X$. If this height rise exceeds $6$m then flooding will occur. The variable $X$ is modelled using the following distribution 
 \begin{displaymath} 
  p(x) =  \left\{\begin{array}{l l} \frac{x}{3} \exp^{-x^2/6} & x \geq 0 \\ 0 & x < 0 \end{array} \right.  
 \end{displaymath}
 Calculate the probability that a flood occurs. 
\end{itemize}
\end{frame}



\begin{frame}{The Bernoulli Distribution}
\begin{itemize}
 \item A (potentially biased) coin lands on heads with probability $p$ and tails with probability $1-p$
\item The outcome follows the \emph{Bernoulli Distribution} 
\item $X$ is a Bernoulli random variable, written $X \sim \mathcal{B}(p)$ if 
\begin{itemize}
\item It is 0 with probability $1-p$ 
\item It is 1 with probability $p$ 
\end{itemize} 
\item The expectation of a Bernoulli random variable is $p$ and the variance is $p(1-p)$
\end{itemize}
\end{frame}


\begin{frame}{Exercise: A Lottery Game}  
\begin{itemize} 
\item Consider a game in which you have to choose 3 numbers correctly from 49 
\begin{itemize}
\item The ``correct'' numbers are chosen uniformly randomly 
\end{itemize}
\item Each ticket costs 1 Euro, and the prize money is 10,000 Euros. 
\item If you keep on playing, can you make a profit? What expected return/loss is there on the ticket?
\end{itemize}
\end{frame}

\begin{frame}{The Binomial Distribution} 
\begin{itemize} 
 \item Number of times one expects to win the previous game follows a \emph{Binomial distribution}  
\item Number of successes of independent observations of Bernoulli random variable form \emph{Binomial random variable} 
\begin{displaymath} 
 P(X = k) = C^k_n p^k (1-p)^{n-k} \quad \forall k = 0, \ldots, n
\end{displaymath}
for success probability $p$, observations $n$ and successes $k$, written $X \sim \mathcal{B}(n, p)$ 
\item In the case $n=1$ we get the Bernoulli distribution 
\item $\sum_k P(X = k) = 1$  
\item Expectation is $np$ and variance is $np(1-p)$
\end{itemize}
\end{frame}

\begin{frame}{Exercise} 
\begin{itemize} 
 \item For the same game, what is the probability of first winning on the 10th day assuming you play once a day? 
 \item Show that the variance of a Bernoulli random variable is $p(1-p)$ 
\end{itemize}
\end{frame}

\begin{frame}{Geometric Distribution} 
\begin{itemize} 
 \item The \emph{Geometric distribution} represents the probability of a first success in a series of Bernoulli trials with success probability $p$ and failure probability $1-p$  
 \begin{displaymath} 
 P(X = k) = p (1-p)^{k-1} \quad  k = 0, \ldots, n
\end{displaymath}
\item We use the notation $X \sim \mathcal{G}(p)$
\item Expectation is $1/p$ and variance is $(1-p)/p^2$
\end{itemize}
\end{frame}

\begin{frame}{Poisson Distribution}  
\begin{itemize} 
 \item \emph{Poisson distribution} represents probabilities of events occurring within time interval given they occur at known average rate and independently of last.  
 \item E.g. We know that buses at a particular stop come every 10 minutes. A Poisson distribution can tell us probability of bus arriving within 5 minute interval. 
 \item $k$ is the frequency of occurrence and $\theta > 0$ is the rate. 
 \begin{displaymath} 
  p_k = P(X = k) = \frac{\theta^k e^{-\theta}}{k!}
 \end{displaymath}
\item We use the notation $X \sim \mathcal{P}(\theta)$
\item Expectation is $\theta$ and variance is $\theta$ 
\end{itemize}
\end{frame}

\begin{frame}{Bus Example}  
\begin{itemize} 
 \item We know that buses at a particular stop come at an average rate of once every 10 minutes. What is the probability of a bus arriving within a 5 minute interval assuming arrival events are independent?
 \item $\theta = 0.5$ and $k = 1$ 
 \item $P(X = k) = 0.303$ 
\end{itemize}
\end{frame}

\begin{frame}{Summary}  
\begin{itemize} 
 \item Formalised random variables 
\item Expectation, variance and their properties 
\item Discrete distributions 
\begin{itemize} 
\item Bernoulli - $p_k= p^k (1-p)^{1-k}$ for $k \in \{0, 1\}$
\item Binomial - $p_k= C^k_n p^k (1-p)^{n-k}$ for $k \in \{0, 1, \ldots, n\}$
\item Geometric - $p_k = p (1-p)^{k-1} $ for $k \in \{1, 2, \ldots\}$
\item Poisson - $p_k =  \frac{\theta^k e^{-\theta}}{k!}$ for $k \in \{0, 1, \ldots\}$
\end{itemize}
\end{itemize} 
\end{frame}


\end{document}