\documentclass{beamer}

\usepackage{beamerthemesplit}
\usepackage[utf8x]{inputenc}
\usepackage{pgf}
\usepackage{default}
\usepackage{url}
\usepackage{subfigure}
\usepackage{algorithmic} 
\usepackage{algorithm} 
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{booktabs}

\usetheme{Singapore}

\input{../include/CsdMacros}
\graphicspath{{./Figures/}}

\title{Statistics and the Analysis of Data\\ Lecture 4: Common Probability Distributions}
\author{Charanpal Dhanjal \\ \texttt{charanpal@gmail.com}} 
\institute{\'{E}cole des Ponts}
\date{26th November 2013}

\begin{document}

\frame{\titlepage}


\begin{frame}{Recap}  
\begin{itemize} 
\item Sample spaces, experiments, events 
\item Independence of events   $P(A \cap B) = P(A)P(B)$
\item Conditional probabilities  $P(A | B) = \frac{P(A \cap B)}{P(B)}$
\item Bayes' Law 
\begin{displaymath}
 P(A | B) = \frac{P(B | A)P(A)}{P(B)}
\end{displaymath}
\end{itemize}
\end{frame}

\begin{frame}{Overview}
\begin{itemize} 
\item Random variables 
\item Probability Distributions
\begin{itemize}
\item Binomial, Bernoulli, Geometric 
\item Poisson, Uniform, Exponential 
\item Gaussian  
\end{itemize}  
\end{itemize}
\end{frame}

\begin{frame}{Probability Density} 
\begin{itemize} 
 \item A more general definition of a random variable is any function $f: \Omega \mapsto \mathbb{R}$ 
\begin{itemize} 
\item $f$ is discrete if it maps to a finite set of values $\{a_1, \ldots, a_m\}$
\item $f$ is continuous if the following is true 
\begin{displaymath} 
P(\{\omega: f(\omega) \in A\}) = \int_{A} p(x) dx, 
\end{displaymath}
for some interval $A$, where $p(x)$ is known as the \emph{probability density function} 
 \end{itemize}
 \end{itemize}
\end{frame}

\begin{frame}{Probability Distributions} 
\begin{itemize}
\item A \emph{probability distribution} defines a random variable 
\item For a discrete variable we have $p_i = P(\{\omega: f(\omega) = a_i\})$
\begin{itemize}
\item In addition, $\sum_i p_i  = 1$
\item Example: for a die $a_i = i$, $i=1,\ldots, 6$ and $p_i = 1/6$
\end{itemize}
\item For a continuous variable the distribution is defined using the density $p(x)$
\begin{itemize}
\item The probability across the compute range is 1: $P(\{\omega: f(\omega) \in \mathbb{R}\}) = \int_\mathbb{R} p(x) dx = 1$ 
\end{itemize}
\item From a probabilistic standpoint, two variables with the same distribution are identical 
\end{itemize}
\end{frame}

\begin{frame}{The Bernoulli Distribution}
\begin{itemize}
 \item A (potentially biased) coin lands on heads with $p$ and tails with probability $1-p$
\item The outcome of the toss follows the \emph{Bernoulli Distribution} 
\item Formally X is a Bernoulli random variable, written $X \sim B(p)$ if 
\begin{itemize}
\item It is 0 with probability $1-p$ 
\item It is 1 with probability $p$ 
\end{itemize} 
\end{itemize}
\end{frame}


\begin{frame}{Exercise: A Lottery Game}  
\begin{itemize} 
\item Consider a game in which you have to choose 3 numbers correctly from 49 
\begin{itemize}
\item The ``correct'' numbers are chosen uniformly randomly 
\end{itemize}
\item Each ticket costs 1 Euro, and the prize money is 10,000 Euros. 
\item If you keep on playing, can you make a profit? What return/loss is there on the ``investment''?
\end{itemize}
\end{frame}

\begin{frame}{The Binomial Distribution} 
\begin{itemize} 
 \item The number of time one expects to win the game follows a \emph{Binomial distribution}  
\item More generally, the number of successes of independent observations of a Bernoulli random variable form a Binomial random variable 
\begin{displaymath} 
 P(X = k) = C^k_n p^k (1-p)^{n-k} \quad \forall k = 0, \ldots, n
\end{displaymath}
for a success probability $p$, number of observations $n$ and number of successes $k$ 
\item This is written as $X \sim B(n, p)$ 
\item In the case $n=1$ we get the Bernoulli disribution 
\item $\sum_k P(X = k) = 1$  
\end{itemize}
\end{frame}

\begin{frame}{Exercise} 
\begin{itemize} 
 \item For the same game, what is the probability of first winning on the 10th day? 
\end{itemize}
\end{frame}

\begin{frame}{Geometric Distribution} 
\begin{itemize} 
 \item The \emph{Geometric distribution} represents the probability of a first success in a series of Bernoulli trials with success probability $p$ and failure probability $1-p$.  
 \begin{displaymath} 
 P(X = k) = p (1-p)^{k-1} \quad \forall k = 0, \ldots, n
\end{displaymath}
\item We use the notation $X \sim \mathcal{G}(p)$
\end{itemize}
\end{frame}

\begin{frame}{Poisson Distribution}  
\begin{itemize} 
 \item The \emph{Poisson Distribution} represents probabilities of events occurring within time interval given they occur at known average rate and independently of last.  
 \item E.g. We know that buses at a particular stop come every 4 minutes. A Poisson distribution can tell us the probability of a bus arriving within a 5 minute interval. 
 \item $k$ is the frequency of occurrence within the interval and $\theta > 0$ is the rate. 
 \begin{displaymath} 
  p_k = P(X = k) = \frac{\theta^k e^{-\theta}}{k!}
 \end{displaymath}
\item We use the notation $X \sim \mathcal{P}(\theta)$
\end{itemize}
\end{frame}

\begin{frame}{Bus Example}  
\begin{itemize} 
 \item We know that buses at a particular stop come every 4 minutes. What is the probability of a bus arriving within a 5 minute interval?
 \item $\theta = 0.5$ and $k = 1$ 
 \item $P(X = k) = 0.303$ 
\end{itemize}
\end{frame}

\begin{frame}{Uniform Distribution}  
 \begin{itemize} 
  \item We have already seem the discrete uniform distribution e.g. when rolling a fair die. 
\item Taken to the limit, we have for an interval $[a, b]$ 
\begin{displaymath} 
 p(x) = \frac{1}{b-a}
\end{displaymath}
\item Denoted by $X \sim \mathcal{U}(a, b)$
\item Note that  $X \sim \mathcal{U}(a, b)$ implies $\frac{X - a}{b - a} \sim \mathcal{U}(0, 1)$
\end{itemize}
\end{frame}

\begin{frame}{More Buses} 
 
\end{frame}


\end{document}